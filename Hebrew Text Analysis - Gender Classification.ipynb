{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assignment 4 - Nlp ex - Labeling gender from hebrew text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# ---------------------------------------\n",
    "import sklearn\n",
    "from sklearn import preprocessing, metrics, pipeline, model_selection, feature_extraction \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "\n",
    "# ----------------- output and visualizations: \n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "# show several prints in one cell. This will allow us to condence every trick in one cell.\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text analysis and String manipulation imports:\n",
    "###### Stop words are not allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Text analysis and Hebrew text analysis imports:\n",
    "# vectorizers:\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# regular expressions:\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding The Data:\n",
    "#### Train Dataset - includes labeled corpus with hebrew stories and target values (gender). \n",
    "   #### - Every story between 300-500 words.\n",
    "#### Test Dataset (not labeled) - been asked to predict the gender of the story writter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>כשחבר הזמין אותי לחול, לא באמת חשבתי שזה יקרה,...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>לפני שהתגייסתי לצבא עשיתי כל מני מיונים ליחידו...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>מאז שהתחילו הלימודים חלומו של כל סטודנט זה הפנ...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>כשהייתי ילד, מטוסים היה הדבר שהכי ריתק אותי. ב...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‏הייתי מדריכה בכפר נוער ומתאם הכפר היינו צריכי...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               story gender\n",
       "0  כשחבר הזמין אותי לחול, לא באמת חשבתי שזה יקרה,...      m\n",
       "1  לפני שהתגייסתי לצבא עשיתי כל מני מיונים ליחידו...      m\n",
       "2  מאז שהתחילו הלימודים חלומו של כל סטודנט זה הפנ...      f\n",
       "3  כשהייתי ילד, מטוסים היה הדבר שהכי ריתק אותי. ב...      m\n",
       "4  ‏הייתי מדריכה בכפר נוער ומתאם הכפר היינו צריכי...      f"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(753, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filename = 'annotated_corpus_for_train.csv'\n",
    "df_train = pd.read_csv(train_filename, index_col=None, encoding='utf-8')\n",
    "df_train.head()\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing punctuations and duplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(749, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "#removing punctuations \n",
    "def remove_punc(text):\n",
    "    for punc in string.punctuation:\n",
    "        text = text.replace(punc, '')\n",
    "    return text\n",
    "#\n",
    "df_train['story'] = df_train['story'].apply(remove_punc)\n",
    "#checking if there are duplicated rows, in case there are any duplicated rows - im only keeping the last one \n",
    "df_train = df_train.drop_duplicates(subset=['story'], keep='last')\n",
    "df_train['story'] = df_train['story'].str.replace('[a-zA-Z0-9]', ' ')\n",
    "#turning the labels of m/f to 0s & 1s\n",
    "df_train['label'] = df_train['gender'].map({'m': 0,'f': 1})\n",
    "\n",
    "df_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m    572\n",
      "f    177\n",
      "Name: gender, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train['gender'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the train Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train(df):\n",
    "    X = df_train['story'].copy()\n",
    "    y = df_train['label'].copy()\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42, shuffle=False ) \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = split_train(df_train)\n",
    "X_train, X_test, y_train, y_test = split_data(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape is: (674,)\n",
      "y_train shape is: (674,)\n",
      "X_test shape is: (75,)\n",
      "y_test shape is: (75,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape is:\", X_train.shape)\n",
    "print(\"y_train shape is:\", y_train.shape)\n",
    "print(\"X_test shape is:\", X_test.shape)\n",
    "print(\"y_test shape is:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I have been asked to get f1 macro score higher than 74:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using a pipeline, built this one with perceptron as my classifier & tfidf - f1 score - 88.\n",
    "\n",
    "train_pipe = Pipeline([\n",
    "\n",
    "    ('vectorizer' , TfidfVectorizer(analyzer = 'word' ,ngram_range=(1, 3),max_df=0.8, token_pattern=r'\\b\\w{2,15}\\b')),\n",
    "    ('clf' , Perceptron(random_state = 42,alpha=0.001, penalty='elasticnet', tol = 1e-7, shuffle = True ,\n",
    "                        eta0 = 0.101))  \n",
    "])\n",
    "train_pipe.fit(X_train,y_train);\n",
    "y_pred = train_pipe.predict(X_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[57,  1],\n",
       "       [ 8,  9]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93        58\n",
      "           1       0.90      0.53      0.67        17\n",
      "\n",
      "    accuracy                           0.88        75\n",
      "   macro avg       0.89      0.76      0.80        75\n",
      "weighted avg       0.88      0.88      0.87        75\n",
      "\n",
      "The f1 marco score is: 0.7967479674796748\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred)\n",
    "clf_rep = metrics.classification_report(y_test,y_pred)\n",
    "print(clf_rep)\n",
    "print('The f1 marco score is:', f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tried using sgd classifier as well - f1 score - 87\n",
    "\n",
    "train_pipe = Pipeline([\n",
    "    \n",
    "    ('vectorizer' , TfidfVectorizer(analyzer = 'word' ,ngram_range=(1, 3),max_df=0.7, token_pattern=r'\\b\\w{3,10}\\b')),\n",
    "    ('clf' , SGDClassifier(random_state = 42,alpha=0.006, loss='perceptron'))\n",
    "    \n",
    "])\n",
    "train_pipe.fit(X_train,y_train);\n",
    "y_pred = train_pipe.predict(X_test);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[55,  3],\n",
       "       [ 7, 10]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92        58\n",
      "           1       0.77      0.59      0.67        17\n",
      "\n",
      "    accuracy                           0.87        75\n",
      "   macro avg       0.83      0.77      0.79        75\n",
      "weighted avg       0.86      0.87      0.86        75\n",
      "\n",
      "The f1 marco score is: 0.7916666666666667\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred)\n",
    "clf_rep = metrics.classification_report(y_test,y_pred)\n",
    "print(clf_rep)\n",
    "print('The f1 marco score is:', f1_score(y_test, y_pred, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
