{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math \n",
    "import statistics\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "# --------------------------------------\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV \n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# --------------------------------------\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.pandas.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "### 3. Preprocessing\n",
    "\n",
    "In this section, there will be implementation of various pre-processing techniques on the \"Loan Risk Factor\" dataset.<br/>\n",
    "<b><u>It will include the following</u></b>:\n",
    "* 3.pre About Dataset \n",
    "* 3.a. Load dataset  \n",
    "* 3.b. Duplicate removal\n",
    "* 3.c. Decouple Dataset\n",
    "* 3.d. Train Test Split\n",
    "* 3.e. Missing Values Handling \n",
    "* 3.f. Outlier Removal\n",
    "* 3.g. Scaling \n",
    "* 3.h. Check Correlation\n",
    "* 3.i. Categorical Columns Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.pre. About Dataset\n",
    "In this section of the assignment you will get familiar with a dataset named \"Loan Risk Factor\" documenting banks' clients asking for a loan.<br/>\n",
    "Each row in the dataset represents a person who takes a credit by a bank.<br/>\n",
    "Each person is classified as **good** or **bad** credit risks according to the following set of attributes:    \n",
    "1. **age** (numeric)\n",
    "2. **sex** (text: male, female)\n",
    "3. **job** (numeric: 0 - unskilled and non-resident, 1 - unskilled and resident, 2 - skilled, 3 - highly skilled)\n",
    "4. **housing** (text: own, rent, or free)\n",
    "5. **years** (numeric, in years)\n",
    "5. **saving_account** (text - little, moderate, quite rich, rich)\n",
    "6. **checking_account** (numeric, in DM - Deutsch Mark)\n",
    "7. **credit_amount** (numeric, in DM)\n",
    "8. **duration** (numeric, in month)\n",
    "9. **purpose** (text: car, furniture/equipment, radio/TV, domestic appliances, repairs, education, business, vacation/others)\n",
    "10. **risk** (target) - (0 - risk, 1 - no risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>job</th>\n",
       "      <th>housing</th>\n",
       "      <th>years</th>\n",
       "      <th>saving_account</th>\n",
       "      <th>checking_account</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>duration</th>\n",
       "      <th>purpose</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>own</td>\n",
       "      <td>1.000</td>\n",
       "      <td>little</td>\n",
       "      <td>rich</td>\n",
       "      <td>1330</td>\n",
       "      <td>12.000</td>\n",
       "      <td>car</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>own</td>\n",
       "      <td>2.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2197</td>\n",
       "      <td>24.000</td>\n",
       "      <td>car</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>female</td>\n",
       "      <td>2</td>\n",
       "      <td>rent</td>\n",
       "      <td>1.250</td>\n",
       "      <td>little</td>\n",
       "      <td>little</td>\n",
       "      <td>1403</td>\n",
       "      <td>15.000</td>\n",
       "      <td>car</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>own</td>\n",
       "      <td>1.500</td>\n",
       "      <td>little</td>\n",
       "      <td>little</td>\n",
       "      <td>2473</td>\n",
       "      <td>18.000</td>\n",
       "      <td>furniture/equipment</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>rent</td>\n",
       "      <td>0.500</td>\n",
       "      <td>little</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2108</td>\n",
       "      <td>6.000</td>\n",
       "      <td>radio/TV</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex  job housing  years saving_account checking_account  \\\n",
       "0   26    male    2     own  1.000         little             rich   \n",
       "1   43    male    2     own  2.000            NaN              NaN   \n",
       "2   28  female    2    rent  1.250         little           little   \n",
       "3   25    male    0     own  1.500         little           little   \n",
       "4   29    male    2    rent  0.500         little              NaN   \n",
       "\n",
       "   credit_amount  duration              purpose  risk  \n",
       "0           1330    12.000                  car 1.000  \n",
       "1           2197    24.000                  car 1.000  \n",
       "2           1403    15.000                  car 1.000  \n",
       "3           2473    18.000  furniture/equipment 0.000  \n",
       "4           2108     6.000             radio/TV   nan  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name_csv = 'data\\loans_risk_factor_dups.csv'\n",
    "load_csv = lambda file_name: pd.read_csv(file_name)\n",
    "risk_factor_df = load_csv(file_name_csv)\n",
    "risk_factor_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1590, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_factor_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### in this assignment i was handed with a dataset named \"Loan Risk Factor\" documenting banks' clients asking for a loan.\n",
    "Each row in the dataset represents a person who takes a credit by a bank.\n",
    "Each person is classified as good or bad credit risks according to a set of attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of duplicated rows is: 1039\n"
     ]
    }
   ],
   "source": [
    "print('number of duplicated rows is:', risk_factor_df.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for duplications\n",
    "def remove_duplicates(df):\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 551 entries, 0 to 1578\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   age               551 non-null    int64  \n",
      " 1   sex               551 non-null    object \n",
      " 2   job               551 non-null    int64  \n",
      " 3   housing           551 non-null    object \n",
      " 4   years             547 non-null    float64\n",
      " 5   saving_account    463 non-null    object \n",
      " 6   checking_account  372 non-null    object \n",
      " 7   credit_amount     551 non-null    int64  \n",
      " 8   duration          547 non-null    float64\n",
      " 9   purpose           551 non-null    object \n",
      " 10  risk              543 non-null    float64\n",
      "dtypes: float64(3), int64(3), object(5)\n",
      "memory usage: 51.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(551, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_duplicates_df = remove_duplicates(risk_factor_df)\n",
    "no_duplicates_df.info()\n",
    "no_duplicates_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting to feature vectors (X) and labels (y)\n",
    "\n",
    "def decouple_data(risk_factor_df):\n",
    "    y = risk_factor_df['risk']\n",
    "    X = risk_factor_df.drop(columns=['risk'])\n",
    "    return X,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inline 8\n"
     ]
    }
   ],
   "source": [
    "#checked how many nulls are there in y column\n",
    "X,y = decouple_data(no_duplicates_df)\n",
    "print(\"inline\", y.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### splitting the data using sklearn train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_data(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42,shuffle=False) \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape  (440, 10)\n",
      "train label shape  (440,)\n",
      "test shape  (111, 10)\n",
      "test label shape  (111,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test =  split_data(X, y)\n",
    "print(\"train shape \",X_train.shape)\n",
    "print(\"train label shape \",y_train.shape)\n",
    "print(\"test shape \",X_test.shape)\n",
    "print(\"test label shape \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### handling missing values of the DataFrame - various ways.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling missing values of the DataFrame with various ways.    \n",
    "def handle_missing_values(X_train, X_test, y_train, y_test):\n",
    "    #been asked to Replace missing values in \"saving_account\" & \"checking_account\" columns with most frequent values\n",
    "    for col in [\"saving_account\", \"checking_account\"]:\n",
    "        X_train[col].fillna(X_train[col].value_counts().index[0], inplace = True)\n",
    "        X_test[col].fillna(X_train[col].value_counts().index[0], inplace = True)\n",
    "    # been asked to Replace missing values in \"credit_amount\" with the median\n",
    "    X_train.credit_amount.fillna(X_train.median(), inplace=True)\n",
    "    X_test.credit_amount.fillna(X_train.median(), inplace=True)\n",
    "  \n",
    "    # Replace missing values in \"years\" & \"duration\" with the average\n",
    "    for col in [\"years\", \"duration\"]:\n",
    "        X_train[col].fillna(X_train[col].mean(), inplace = True)\n",
    "        X_test[col].fillna(X_train[col].mean(), inplace = True)\n",
    "\n",
    "    # Delete Rows with missing values in \"risk\" column\n",
    "    y_train.dropna(axis=0, how = all, inplace = True)\n",
    "    y_test.dropna(axis=0, how = all, inplace = True)\n",
    " \n",
    "    # the Return of the datasets after implementing these steps\n",
    "    X_train = X_train.loc[(X_train.index.isin(y_train.index))]\n",
    "    X_test = X_test.loc[(X_test.index.isin(y_test.index))]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filled, X_test_filled, y_train_filled, y_test_filled = handle_missing_values(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(432, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(111, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(432,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(111,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_filled.shape\n",
    "X_test_filled.shape\n",
    "y_train_filled.shape\n",
    "y_test_filled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing outliers in age, years, credit_amount & duration colomns using IQR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    X_train_clean = X_train.copy()\n",
    "    X_test_clean = X_test.copy()\n",
    "    for col in [\"age\", \"years\", \"credit_amount\", \"duration\"]:\n",
    "        Q1 = np.percentile(X_train[col], 25)\n",
    "        Q3 = np.percentile(X_train[col], 75)\n",
    "        IQR = Q3 - Q1\n",
    "        X_train_clean.loc[(X_train_clean[col] < Q1 - 1.5*IQR) | (X_train_clean[col] > Q3 + 1.5*IQR ),col] = np.nan\n",
    "        X_test_clean.loc[(X_test_clean[col] < Q1 - 1.5*IQR) | (X_test_clean[col] > Q3 + 1.5*IQR ),col] = np.nan\n",
    "\n",
    "    X_train_clean.dropna(inplace = True)\n",
    "    X_test_clean.dropna(inplace = True)\n",
    "    y_train = y_train[list(X_train_clean.index)]\n",
    "    y_test = y_test[list(X_test_clean.index)]\n",
    "\n",
    "    return X_train_clean, X_test_clean, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_no_out , X_test_no_out, y_train, y_test = remove_outliers(X_train_filled, X_test_filled, y_train_filled, y_test_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new shapes after removing outliers are:\n",
      "\n",
      "train features (377, 10)\n",
      "train label  (377,)\n",
      "test features  (93, 10)\n",
      "test label  (93,)\n"
     ]
    }
   ],
   "source": [
    "print('The new shapes after removing outliers are:\\n')\n",
    "print(\"train features\",X_train_no_out.shape)\n",
    "print(\"train label \",y_train.shape)\n",
    "print(\"test features \",X_test_no_out.shape)\n",
    "print(\"test label \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaling the columns above in the range [0,1] using the Sklearn MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale_data(X_train, X_test):\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    minmax_X_train = X_train.copy()\n",
    "    minmax_X_test = X_test.copy()\n",
    "\n",
    "    for col in [\"age\", \"years\", \"credit_amount\", \"duration\"]:\n",
    "        minmax_X_train[col] = scaler.fit_transform(X_train[[col]])\n",
    "        minmax_X_test[col] = scaler.transform(X_test[[col]])\n",
    "        \n",
    "    return minmax_X_train, minmax_X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_X_train, minmax_X_test = scale_data(X_train_no_out, X_test_no_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing correlated columns when correlation value > 0.95, removing the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_correlated(X_train, X_test):\n",
    "\n",
    "    X_train_corr = X_train.copy()\n",
    "    correlated_features = []\n",
    "    correlation_matrix = X_train_corr.corr()\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > 0.95:\n",
    "                colname = correlation_matrix.columns[i]\n",
    "                correlated_features.append(colname)\n",
    "                \n",
    "    X_train = X_train.drop(correlated_features, axis = 1)\n",
    "    X_test = X_test.drop(correlated_features, axis = 1)            \n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_no_corr, X_test_no_corr = remove_correlated(minmax_X_train, minmax_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(377, 9)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(93, 9)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_no_corr.shape\n",
    "X_test_no_corr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling categorical columns using dummies \n",
    "\n",
    "def handle_categorical_columns(X_train, X_test):\n",
    "    X_train_object = list(X_train.select_dtypes(include=['object']))\n",
    "    X_train_dum = pd.get_dummies(data = X_train,columns = X_train_object)\n",
    "    X_test_dum = pd.get_dummies(data = X_test,columns = X_train_object)\n",
    "    X_test_dum = X_test_dum.reindex(columns = X_train_dum.columns, fill_value=0)\n",
    "    \n",
    "    return X_train_dum, X_test_dum\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numeric, X_test_numeric = handle_categorical_columns(X_train_no_corr, X_test_no_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(377, 24)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(93, 24)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_numeric.shape\n",
    "X_test_numeric.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Been asked to try 3 different classifiers: Knn, Naive Bayes & Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Knn with 3 Neighbours\n",
    "\n",
    "def train_knn(X_train, y_train):\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn_clf = knn.fit(X_train, y_train)\n",
    "    return knn_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_knn = train_knn(X_train_numeric, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "\n",
    "def train_naive_bayes(X_train, y_train):\n",
    "    gnb = GaussianNB()\n",
    "    # Train classifier\n",
    "    clf = gnb.fit(X_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf = train_naive_bayes(X_train_numeric, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Classifier with max depth of 3 and min split of 20 \n",
    "def train_decision_tree(X_train, y_train):\n",
    "    tree = DecisionTreeClassifier(max_depth =3,min_samples_split = 20)\n",
    "    clf = tree.fit(X_train, y_train)\n",
    "    return clf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = train_decision_tree(X_train_numeric, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy evaluation\n",
    "\n",
    "def evaluate_accuracy(clf, X_test, y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy evaluation for each classifier:\n",
      "\n",
      "Knn Classifier:\n",
      " 0.5053763440860215\n",
      "Naive Bayes Classifier:\n",
      " 0.6344086021505376\n",
      "Decision Tree Classifier:\n",
      " 0.5268817204301075\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy evaluation for each classifier:\\n')\n",
    "print('Knn Classifier:\\n', evaluate_accuracy(clf_knn, X_test_numeric, y_test))\n",
    "print('Naive Bayes Classifier:\\n', evaluate_accuracy(nb_clf, X_test_numeric, y_test))\n",
    "print('Decision Tree Classifier:\\n', evaluate_accuracy(dt_clf, X_test_numeric, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Been asked to calculate 10-fold cross-validation mean accuracy score\n",
    "\n",
    "def evaluate_accuracy_cross_validation(clf, X, y):\n",
    "    mean_accuracy = cross_val_score(clf, X, y, cv = 10)\n",
    "    return mean_accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters Tuning - knn grid search with the following params:\n",
    "#'n_neighbors' with values [3, 5, 11, 20, 35, 50] \n",
    "#'weights' with values ['uniform', 'distance']\n",
    "#'metric' with values ['euclidean', 'manhattan', 'minkowski']\n",
    "\n",
    "\n",
    "def grid_search_knn(clf, X_train, y_train):\n",
    "\n",
    "    # Creating the hyperparameter grid\n",
    "    param_grid = {'n_neighbors': [3, 5, 11, 20, 35, 50], 'weights':['uniform', 'distance'], 'metric':['euclidean', 'manhattan', 'minkowski']}\n",
    "\n",
    "    # Instantiating the GridSearchCV object\n",
    "    grid_knn = GridSearchCV(clf, param_grid, cv = 5)\n",
    "\n",
    "    grid_knn.fit(X_train, y_train)\n",
    "    \n",
    "    Y = grid_knn.best_params_\n",
    "    \n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metric': 'euclidean', 'n_neighbors': 35, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "Knn_grid_result = grid_search_knn(KNeighborsClassifier(), X_train_numeric, y_train)\n",
    "print(Knn_grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knn_by_Gs(X_train, y_train):\n",
    "    knn = KNeighborsClassifier(n_neighbors=35,weights = 'uniform',metric = 'euclidean')\n",
    "    knn_clf1 = knn.fit(X_train, y_train)\n",
    "    return knn_clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knn Classifier:\n",
      " 0.5806451612903226\n"
     ]
    }
   ],
   "source": [
    "clf_knn_gs = train_knn_by_Gs(X_train_numeric, y_train)\n",
    "print('Knn Classifier:\\n', evaluate_accuracy(clf_knn_gs, X_test_numeric, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters Tuning - Decision Tree grid search with the following params\n",
    "#'criterion' with the values ['gini', 'entropy']\n",
    "#'max_depth' with the values [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "#'min_samples_split' with the values [5, 10, 15, 20, 25]\n",
    "#'min_samples_leaf' with the values [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "\n",
    "def grid_search_decision_tree(clf, X_train, y_train):\n",
    "    param_grid = {'criterion': ['gini', 'entropy'], 'max_depth':[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \n",
    "                  'min_samples_split':[5, 10, 15, 20, 25],'min_samples_leaf':[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "    \n",
    "    # Instantiating the GridSearchCV object\n",
    "    grid_dt = GridSearchCV(clf, param_grid, cv = 5)\n",
    "\n",
    "    grid_dt.fit(X_train, y_train)\n",
    "    \n",
    "    Y = grid_dt.best_params_\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 15}\n"
     ]
    }
   ],
   "source": [
    "dt_grid_result = grid_search_decision_tree(DecisionTreeClassifier(), X_train_numeric, y_train)\n",
    "print(dt_grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree_by_gs(X_train, y_train):\n",
    "    tree = DecisionTreeClassifier(max_depth =10,min_samples_split = 15,min_samples_leaf = 1,criterion = 'gini')\n",
    "    clf1 = tree.fit(X_train, y_train)\n",
    "    return clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knn Classifier:\n",
      " 0.5698924731182796\n"
     ]
    }
   ],
   "source": [
    "dt_clf_gs = train_decision_tree_by_gs(X_train_numeric, y_train)\n",
    "print('Knn Classifier:\\n', evaluate_accuracy(dt_clf_gs, X_test_numeric, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
